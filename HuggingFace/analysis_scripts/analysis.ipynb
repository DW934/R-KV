{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rkv.monkeypatch import replace_llama, replace_qwen2, replace_qwen3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt_template = \"You are given a math problem.\\n\\nProblem: {question}\\n\\n You need to solve the problem step by step. First, you need to provide the chain-of-thought, then provide the final answer.\\n\\n Provide the final answer in the format: Final answer:  \\\\boxed{{}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"You are given a math problem.\\n\\nProblem: {question}\\n\\n You need to solve the problem step by step. First, you need to provide the chain-of-thought, then provide the final answer.\\n\\n Provide the final answer in the format: Final answer:  \\\\boxed{{}}\"\n",
    "\n",
    "with open(\"../data/gsm8k.jsonl\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "sample_id = 1281\n",
    "sample = test_data[sample_id]\n",
    "prompt = prompt_template.format(question=sample[\"question\"])\n",
    "answer = sample[\"answer\"].split(\"####\")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Model\n",
    "\n",
    "Choose from:\n",
    "- deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
    "- deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
    "- deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n",
    "- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Configuration\n",
    "\n",
    "- Choose method from: rkv, snapkv, streamingllm, h2o, analysiskv\n",
    "- Choose budget from: 128, 256, 512, 1024\n",
    "- Choose mix_lambda from 0 to 1.\n",
    "\n",
    "- When mix_lambda=0, the selection is dominated by redundency.\n",
    "- When mix_lambda=1, the selection is dominated by attention.\n",
    "\n",
    "- Analysiskv is a special method that we do not compress KV but only return the selection patterns. In this way, we could observe the importance score without compressing KV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_config = {\n",
    "    \"method\": \"rkv\",\n",
    "    \"method_config\": {\n",
    "        \"budget\": 128,\n",
    "        \"window_size\": 8,\n",
    "        \"mix_lambda\": 0.07,\n",
    "        \"retain_ratio\": 0.2,\n",
    "        \"retain_direction\": \"last\",\n",
    "        \"record_kept_token_indices\": True,\n",
    "    },\n",
    "    \"compression\": None,\n",
    "    \"update_kv\": True\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"divide_method\": \"newline\",\n",
    "    \"divide_length\": 128,\n",
    "    \"compression_content\": \"all\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, use_fast=True, padding_side=\"left\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "if 'qwen3' in model_name.lower():\n",
    "    replace_qwen3(compression_config)\n",
    "elif 'llama' in model_name.lower():\n",
    "    replace_llama(compression_config)\n",
    "else:  # qwen2\n",
    "    replace_qwen2(compression_config)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").eval()\n",
    "\n",
    "model.config.update(model_config)\n",
    "\n",
    "model.newline_token_ids = [\n",
    "    tokenizer.encode(\"\\n\")[-1],\n",
    "    tokenizer.encode(\".\\n\")[-1],\n",
    "    tokenizer.encode(\")\\n\")[-1],\n",
    "    tokenizer.encode(\"\\n\\n\")[-1],\n",
    "    tokenizer.encode(\".\\n\\n\")[-1],\n",
    "    tokenizer.encode(\")\\n\\n\")[-1],\n",
    "]\n",
    "\n",
    "model.after_think_token_ids = [\n",
    "    tokenizer.encode(\"</think>\")[-1],\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda:4\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=8192,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "## Display model answer\n",
    "print(\"Sample ID: \", sample_id)\n",
    "print(\n",
    "    tokenizer.decode(outputs[0], skip_special_tokens=True),\n",
    ")\n",
    "print(\"\\n\\nGround Truth: \", answer)\n",
    "print(\"Generation length:\", len(outputs[0]) - inputs.input_ids.shape[1])\n",
    "print(\n",
    "    \"Compression Steps:\",\n",
    "    len(model.model.layers[-1].self_attn.kv_cluster.kept_token_indices),\n",
    ")\n",
    "print(\"Evicted tokens:\", model.model.layers[-1].self_attn.kv_cluster.evicted_token_num)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize token eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the token eviction pattern for a given head for one compression step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rkv.utils import visualize_token_eviction\n",
    "\n",
    "layer_id = 31\n",
    "head_id = 6\n",
    "step_idx = 5\n",
    "\n",
    "kept_indices_lst = model.model.layers[layer_id].self_attn.kv_cluster.kept_token_indices\n",
    "visualize_token_eviction(\n",
    "    outputs[0], kept_indices_lst[step_idx], tokenizer, head_idx=head_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the token eviction pattern for a given heads at each compression step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rkv.utils import visualize_multistep_token_eviction\n",
    "\n",
    "layer_id = 31\n",
    "head_id = 6\n",
    "step_idx = 5\n",
    "\n",
    "kept_indices_lst = model.model.layers[layer_id].self_attn.kv_cluster.kept_token_indices\n",
    "visualize_multistep_token_eviction(\n",
    "    outputs[0], kept_indices_lst, tokenizer, head_idx=head_id, step_idx=step_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the token eviction pattern for all heads at each compression step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rkv.utils import visualize_multistep_token_eviction_by_head\n",
    "\n",
    "layer_id = 31\n",
    "step_idx = 5\n",
    "\n",
    "kept_indices_lst = model.model.layers[layer_id].self_attn.kv_cluster.kept_token_indices\n",
    "\n",
    "print(\"Total Step: \", len(kept_indices_lst))\n",
    "visualize_multistep_token_eviction_by_head(\n",
    "    outputs[0], kept_indices_lst, tokenizer, step_idx=step_idx, aggregate=True\n",
    ")\n",
    "\n",
    "# aggregate: when set to False, later heads will cover previous heads. when set to `True`, will compute how many times a token are covered by a head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the token eviction score for all heads at each compression step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rkv.utils import visualize_multistep_token_eviction_score_by_head\n",
    "\n",
    "layer_id = 31\n",
    "head_id = 6\n",
    "step_idx = 5\n",
    "\n",
    "kept_indices_lst = model.model.layers[layer_id].self_attn.kv_cluster.kept_token_indices\n",
    "kept_attention_scores_lst = model.model.layers[layer_id].self_attn.kv_cluster.kept_attention_scores\n",
    "\n",
    "visualize_multistep_token_eviction_score_by_head(\n",
    "    outputs[0], kept_indices_lst, kept_attention_scores_lst, tokenizer, step_idx=step_idx, head_idx=head_idx,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
