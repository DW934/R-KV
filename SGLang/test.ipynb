{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs fullkv on GSM 8K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import sglang as sgl\n",
    "\n",
    "os.makedirs(\"./z_experiment/evaluation/\", exist_ok=True)\n",
    "os.makedirs(\"./z_experiment/output/\", exist_ok=True)\n",
    "os.makedirs(\"./z_experiment/results/\", exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "    save_path: str = \"./z_experiment/output/test_output.jsonl\"\n",
    "    data_size: int = 10\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "PROMPT_TEMP = \"You are given a math problem.\\n\\nProblem: {question}\\n\\n You need to solve the problem step by step. First, you need to provide the chain-of-thought, then provide the final answer.\\n\\n Provide the final answer in the format: Final answer:  \\\\boxed{{}}\"\n",
    "\n",
    "test_data = []\n",
    "prompts = []\n",
    "with open(\"./evaluation/data/test_one.jsonl\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == config.data_size:\n",
    "            break\n",
    "        sample = json.loads(line)\n",
    "        prompt = PROMPT_TEMP.format(question=sample[\"question\"])\n",
    "        prompts.append(prompt)\n",
    "\n",
    "        sample[\"prompt\"] = prompt\n",
    "        sample[\"index\"] = idx\n",
    "\n",
    "        test_data.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_algorithm = \"RKV\" # \"RKV\"\n",
    "compress_max_window = 8\n",
    "compress_max_prompt = 128\n",
    "compress_divide_length = 64\n",
    "compress_divide_method = \"step_length\" # \"step_length\", \"newline\"\n",
    "\n",
    "llm = sgl.Engine(\n",
    "    model_path=config.model_path,\n",
    "    dtype=\"bfloat16\",\n",
    "    disable_overlap_schedule=True,\n",
    "    compress_algorithm=compress_algorithm,\n",
    "    compress_max_window=compress_max_window,\n",
    "    compress_max_prompt=compress_max_prompt,\n",
    "    compress_divide_length=compress_divide_length,\n",
    "    compress_divide_method=compress_divide_method,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    sampling_params = {\"temperature\": 0.0, \"top_p\": 0.95, \"max_new_tokens\": 8192}\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    end_time = time.time()\n",
    "\n",
    "    llm.shutdown()\n",
    "\n",
    "    for sample_idx, output in enumerate(outputs):\n",
    "        test_data[sample_idx][\"output\"] = output[\"text\"]\n",
    "        test_data[sample_idx][\"prefill_tokens\"] = output[\"meta_info\"][\"prompt_tokens\"]\n",
    "        test_data[sample_idx][\"output_tokens\"] = output[\"meta_info\"][\n",
    "            \"completion_tokens\"\n",
    "        ]\n",
    "        test_data[sample_idx][\"total_tokens\"] = (\n",
    "            output[\"meta_info\"][\"prompt_tokens\"]\n",
    "            + output[\"meta_info\"][\"completion_tokens\"]\n",
    "        )\n",
    "\n",
    "    with open(config.save_path, \"w\") as fp:\n",
    "        for line in test_data:\n",
    "            fp.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    total_tokens_generated = sum(\n",
    "        output[\"meta_info\"][\"completion_tokens\"] for output in outputs\n",
    "    )\n",
    "    throughput_tokens = total_tokens_generated / total_time\n",
    "    throughput_requests = len(prompts) / total_time\n",
    "\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Throughput (tokens/s): {throughput_tokens:.2f}\")\n",
    "    print(f\"Throughput (requests/s): {throughput_requests:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = prompts[:100]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Use the following commands to evaluate GSM8k results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python evaluation/math_eval_all_v2.py \\\n",
    "    --exp_name \"evaluation\" \\\n",
    "    --output_dir \"./z_experiment/results\" \\\n",
    "    --base_dir \"./z_experiment/output\" \\\n",
    "    --dataset gsm8k\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
